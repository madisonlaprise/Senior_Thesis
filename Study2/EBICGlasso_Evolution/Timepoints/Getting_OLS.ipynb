{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns before dropping: ['Qualtricsname', 'Participant_Number', 'Corona', 'ResponseID', 'Worker_ID', 'StartDate', 'EndDate', 'Durationinseconds', 'Age', 'Gender', 'Education_version', 'Education_years_A', 'Relationship_status', 'Sexual_attraction', 'Children_YN', 'Children_num', 'Nationality', 'Ethnicity', 'LSAS_anx1', 'LSAS_anx2', 'LSAS_anx3', 'LSAS_anx4', 'LSAS_anx5', 'LSAS_anx6', 'LSAS_anx7', 'LSAS_anx8', 'LSAS_anx9', 'LSAS_anx10', 'LSAS_anx11', 'LSAS_anx12', 'LSAS_anx13', 'LSAS_anx14', 'LSAS_anx15', 'LSAS_anx16', 'LSAS_anx17', 'LSAS_anx18', 'LSAS_anx19', 'LSAS_anx20', 'LSAS_anx21', 'LSAS_anx22', 'LSAS_anx23', 'LSAS_anx24', 'LSAS_avo1', 'LSAS_avo2', 'LSAS_avo3', 'LSAS_avo4', 'LSAS_avo5', 'LSAS_avo6', 'LSAS_avo7', 'LSAS_avo8', 'LSAS_avo9', 'LSAS_avo10', 'LSAS_avo11', 'LSAS_avo12', 'LSAS_avo13', 'LSAS_avo14', 'LSAS_avo15', 'LSAS_avo16', 'LSAS_avo17', 'LSAS_avo18', 'LSAS_avo19', 'LSAS_avo20', 'LSAS_avo21', 'LSAS_avo22', 'LSAS_avo23', 'LSAS_avo24', 'BDI_1', 'BDI_2', 'BDI_3', 'BDI_4', 'BDI_5', 'BDI_6', 'BDI_7', 'BDI_8', 'BDI_9', 'BDI_10', 'BDI_11', 'BDI_12', 'BDI_13', 'BDI_14', 'BDI_15', 'BDI_16', 'BDI_17', 'BDI_18', 'BDI_19', 'BDI_20', 'CFS_1', 'CFS_2_R', 'CFS_3_R', 'CFS_4', 'CFS_5_R', 'CFS_6', 'CFS_7', 'CFS_8', 'CFS_9', 'CFS_10_R', 'CFS_11', 'CFS_12', 'YearMonth', 'LSAS_1', 'LSAS_2', 'LSAS_3', 'LSAS_4', 'LSAS_5', 'LSAS_6', 'LSAS_7', 'LSAS_8', 'LSAS_9', 'LSAS_10', 'LSAS_11', 'LSAS_12', 'LSAS_13', 'LSAS_14', 'LSAS_15', 'LSAS_16', 'LSAS_17', 'LSAS_18', 'LSAS_19', 'LSAS_20', 'LSAS_21', 'LSAS_22', 'LSAS_23', 'LSAS_24']\n",
      "Columns after dropping: ['Qualtricsname', 'Participant_Number', 'Corona', 'ResponseID', 'Worker_ID', 'StartDate', 'EndDate', 'Durationinseconds', 'Age', 'Gender', 'Education_version', 'Education_years_A', 'Relationship_status', 'Sexual_attraction', 'Children_YN', 'Children_num', 'Nationality', 'Ethnicity', 'BDI_1', 'BDI_2', 'BDI_3', 'BDI_4', 'BDI_5', 'BDI_6', 'BDI_7', 'BDI_8', 'BDI_9', 'BDI_10', 'BDI_11', 'BDI_12', 'BDI_13', 'BDI_14', 'BDI_15', 'BDI_16', 'BDI_17', 'BDI_18', 'BDI_19', 'BDI_20', 'CFS_1', 'CFS_2_R', 'CFS_3_R', 'CFS_4', 'CFS_5_R', 'CFS_6', 'CFS_7', 'CFS_8', 'CFS_9', 'CFS_10_R', 'CFS_11', 'CFS_12', 'YearMonth', 'LSAS_1', 'LSAS_2', 'LSAS_3', 'LSAS_4', 'LSAS_5', 'LSAS_6', 'LSAS_7', 'LSAS_8', 'LSAS_9', 'LSAS_10', 'LSAS_11', 'LSAS_12', 'LSAS_13', 'LSAS_14', 'LSAS_15', 'LSAS_16', 'LSAS_17', 'LSAS_18', 'LSAS_19', 'LSAS_20', 'LSAS_21', 'LSAS_22', 'LSAS_23', 'LSAS_24']\n",
      "LSAS columns: ['LSAS_1', 'LSAS_2', 'LSAS_3', 'LSAS_4', 'LSAS_5', 'LSAS_6', 'LSAS_7', 'LSAS_8', 'LSAS_9', 'LSAS_10', 'LSAS_11', 'LSAS_12', 'LSAS_13', 'LSAS_14', 'LSAS_15', 'LSAS_16', 'LSAS_17', 'LSAS_18', 'LSAS_19', 'LSAS_20', 'LSAS_21', 'LSAS_22', 'LSAS_23', 'LSAS_24']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "cleaned_data = pd.read_csv(r'Study2_data.csv')\n",
    "\n",
    "# Convert StartDate to datetime and extract Year-Month\n",
    "cleaned_data['StartDate'] = pd.to_datetime(cleaned_data['StartDate'])\n",
    "cleaned_data['YearMonth'] = cleaned_data['StartDate'].dt.to_period('M')\n",
    "\n",
    "# Combine LSAS_avo and LSAS_anx into LSAS_1, LSAS_2, ..., LSAS_24\n",
    "for i in range(1, 25):\n",
    "    avo_col = f'LSAS_avo{i}'\n",
    "    anx_col = f'LSAS_anx{i}'\n",
    "    combined_col = f'LSAS_{i}'\n",
    "    cleaned_data[combined_col] = cleaned_data[avo_col] + cleaned_data[anx_col]\n",
    "\n",
    "# Drop LSAS_avo_* and LSAS_anx_* columns\n",
    "cols_to_drop = [f'LSAS_avo{i}' for i in range(1, 25)] + [f'LSAS_anx{i}' for i in range(1, 25)]\n",
    "print(\"Columns before dropping:\", cleaned_data.columns.tolist())\n",
    "cleaned_data = cleaned_data.drop(columns=cols_to_drop, errors='raise')  # Use 'raise' to ensure it fails if columns aren't found\n",
    "print(\"Columns after dropping:\", cleaned_data.columns.tolist())\n",
    "\n",
    "# Ensure only combined LSAS columns are used\n",
    "lsas_columns = [f'LSAS_{i}' for i in range(1, 25)]\n",
    "print(\"LSAS columns:\", lsas_columns)\n",
    "\n",
    "# Rename BDI1 to BDI_1, BDI2 to BDI_2, ..., BDI21 to BDI_21\n",
    "for i in range(1, 22):\n",
    "    old_col = f'BDI{i}'\n",
    "    new_col = f'BDI_{i}'\n",
    "    cleaned_data.rename(columns={old_col: new_col}, inplace=True)\n",
    "\n",
    "# Define other question columns\n",
    "bdi_columns = [col for col in cleaned_data.columns if 'BDI' in col]\n",
    "cfs_columns = [col for col in cleaned_data.columns if 'CFS' in col]\n",
    "\n",
    "# Reverse score CFS questions 2, 3, 5, 10 in place\n",
    "reverse_columns = ['CFS_2_R', 'CFS_3_R', 'CFS_5_R', 'CFS_10_R']\n",
    "\n",
    "# Apply the reverse scoring transformation in place\n",
    "cleaned_data[reverse_columns] = 7 - cleaned_data[reverse_columns]\n",
    "\n",
    "# Reverse the DIRECTION of all CFS columns\n",
    "cleaned_data[cfs_columns] *= -1\n",
    "\n",
    "# Combine all question columns into one list\n",
    "question_columns = lsas_columns + bdi_columns + cfs_columns\n",
    "print(\"Question columns for melt:\", question_columns)\n",
    "\n",
    "# Reshape the data so that each row is a unique observation\n",
    "# Each row should have a Participant_Number, StartDate, EndDate, Question, and Score\n",
    "long_data = pd.melt(cleaned_data, \n",
    "                    id_vars=['Participant_Number', 'YearMonth'], \n",
    "                    value_vars=question_columns, \n",
    "                    var_name='Question', value_name='Score')\n",
    "\n",
    "# Aggregate observations within the same time point by the mean\n",
    "# Create a dictionary of unique participants for each time point\n",
    "participant_dict = long_data.groupby('YearMonth')['Participant_Number'].apply(set).to_dict()\n",
    "long_data= (\n",
    "    long_data.groupby(['Participant_Number', 'YearMonth', 'Question'])['Score']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "long_data['Score'] = (long_data['Score'] - long_data['Score'].mean()) / long_data['Score'].std()\n",
    "\n",
    "long_data_checkpoint = long_data\n",
    "df = long_data_checkpoint\n",
    "\n",
    "# Select three months based on participant counts and temporal coverage\n",
    "selected_months = ['2020-01', '2020-05', '2020-09']\n",
    "\n",
    "# Filter the dataset for the selected months\n",
    "df['YearMonth'] = df['YearMonth'].astype(str)  # Convert YearMonth to string for filtering\n",
    "filtered_df = df[df['YearMonth'].isin(selected_months)]\n",
    "\n",
    "# Aggregate data by participant, month, and test type\n",
    "aggregated_df = (\n",
    "    filtered_df.groupby(['Participant_Number', 'YearMonth', 'Question'])['Score']\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 6: Reshape data into wide format\n",
    "wide_df = aggregated_df.pivot_table(\n",
    "    index='Participant_Number',\n",
    "    columns=['YearMonth', 'Question'],\n",
    "    values='Score'\n",
    ").reset_index()\n",
    "\n",
    "# Flatten multi-index columns\n",
    "wide_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in wide_df.columns]\n",
    "\n",
    "# Make sure Participant_Number_ is renamed to Participant_Number\n",
    "wide_df.rename(columns={'Participant_Number_': 'Participant_Number'}, inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# Save the reshaped dataset if needed\n",
    "wide_df.to_csv('OLS_ready.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
